- What is Generative AI?
    - Generative Advisarial Networks (GANs)
    - Transformer Models

- Capabilities across different functions
    - Text Generation
    - Image Creation
    - Audio Production
    - Code Generation
    - Data Synthasis

- Capabilities of Generative AI for Business Functions
    - Data Collection
    - Data Preprocessing 
    - Model Training
    - Model Optimizaton
    - Content Generation
    - Evaluation

- Technical Foundations of Generative AI
    - (GANs)
        - Involve two neutal networks, the generator and the discriminator, 
            working in tandem to profuce hightly realistic outputs.
                - [Latent Space] -> Noise -> Generator -> Fake Sample
                - Real Samples -> Discriminator -> Real
    
     Transformer Models (GPT and BERT)
        - Use Attention mehanisms to create text that is contextually relevant 
            and stylistically coherent.
        - Focus Selectively
        - Highlight Information
        - Coherent 
        - Contextually Appropriate
    
- Human design choices, Engineering and Oversight
    [Training Data] -> [Machine Learning] -> [Model + Inputs: Producton data/Information] 
        -> [Processing] -> {Continuous Learning -> Machine Learning} ->
        Outputs: Predictions, Actions, Recommendations, and Decisions.
    
- Artificial Intelligence 
    - AI is the field of computing focued on creating systems capbable of performing tasks
        that wold typically require human intetlligence.These tasks include reasoning, learning,
        problem-solving, perception, langueage understanding, and even the ability to move 
        and manipulate objects.
    - AI technologies leverage algorithms and dynamic computing enviroments to enable machines to solve
        complex problems, adapt to new situations, and learn from past experiences.
    - Central to AI is Maching Learning (ML), where algorithms detect patterns and infer
        probabilities from data, allowing the machine to improve its performance over time.
    - AT systems can range from simple, rule-based algorithms to complex neural networks modeled on 
        the human brain.

- Machine Learning (ML) s
    - (ML) is a critical domain within artifical inteligence that emphasized the development of
        algorithms and statistical models that enable computers to perform specific tasks without
        explicit instructions. 
    - Instead, these systems learn and make predictions or decisions based on data.

    - Types of learning:
        - Supervised Learning:
            - Algorithms learn from labeled training data, aiming predict outcomes for
                new inputs.
        - Unsupervised Learning:   
            - Algrothms identify patterns in data without needing labeled responces,
                often used for clustering and association.
        - Reinforcedment Learning: 
            - Models learn to make sequences fo decisions by reveiving feedback on the 
                actions' effectieness.

    - Algorithms and Techniques:
        - Common algorithms include linear regression, decision trees, and neural networks.
        - Advaned techniques involve deep learning, which uses layered neural networks to
            analyze various levels of data features.

    - Data hangling and Processing:
        - Effective machine learning requires robust data preprocessing including normalization,
            handling missing values, and feature selection to improve model accuracy.

    - Performance evaluation:
        - ML models are evaluated based on metrics such as accuracy, precision, recall, and 
            the area under the receiver operating characteristic (ROC) curve, ensuring that they 
            perfrom well on unseen data.

    - Application Areas:
        - ML is applied in various fields such as finace for algorithmic trading, healthcare 
            for redictive diagnostics, and autonomous vehicls for navigation systems.

- Deep Learning:
    - (DL) is an advanced branch of (ML) that uses artifical neural networks with multiple layers,
        know as deep neural networks. These  networks are cabale of learing from large amounts of
        unstructured data.
    - (DL) Models automatically extract and learn features at multiple levels of abstraction, 
        enabling the system to learn complex patterns in large datasets.
    - Learning Process:
        - Supervised: Wehre the model is trained with labeled data
        - Semi-Supervised: which uses a mix of labeled and unlabeled data.
        - Unsupervised- Which relies solely on unlabeled data
    - This technique is particularly effective in areas such as imae recognition, 
        natural language processing (NLP), and speech recognition, where conventional maching-learning
        may fall short due to the data structures' complexity.
    - (DL) has propelled advancements in generative AI, enabling the creation of
        sophisticated models like generative adversarial networks (GANs) that an generate
        new data instances that mimic real data.

- Neural Networks
    - (NN) are compromised of AI. They are particulary effective in pattern recognitoin and data
        interpretation tasks, which they achieve through a structure inspired by the human brain.
        Comprising layers of interconnected nodes or neurons, each with its weights and biases, 
        NN processes input data through these nodes.
    - The connections between nodes represent synapses and are weighted according to their importance.
    - As data passes through each layer, the network adjusts the weights, which is how learning occurs.
    - This structure enables neural networks to learn from vast amounts of data to make decisions, 
        classify data, or predict outcomes with high accuracy.
    - (NN) are particulaly crucial in fields such as computer vision, speech recognition,
        and NLP where they can recognize complex patterns and naunces better then traditional
        algorithms.
    - The training process involves techniques such a backpropagation, where the model learns to minimize
        errors by adjusting weights to produce the most accurate outputs possible.

- Generavtive Adversrial Networks (GAN)
    - GANs are a sophisticated class of AI algorithms used in ML, characterized by their unique
        structure fo two completing NNs: the generator and the discriminator.
    - The Generator is tasked with creating data that is indistinguishable from 
        genuine data.
    - The Discriminator evaluates wheter the generated data is real or fake.
    - This Adversarial process, much like a teacher-student dynamic, continuously improves
        the accuracy of the generated outputs. The training involves the discriminator
        learning to better distinguish between real and generated data, while the generator
        strives to produce increasingly convincing data, enhancing its ability to deceive
        the discriminator.
    - This setup not only helps in generating new dat samples but is also useful in unsupervised
        learning, semi-supervised learning, and reinforcement learning.
    - (GANs) are particualrly renowned for their applications in image generation, video creation,
        and voice synthesis, where they and produce highly realistic outputs.

- Natural Language Processing (NLP)
    - (NLP) is an advance area of AI that focuses on the interaction between computers and humans 
        through natural language.
    - The goals of (NLP) is to read, decipher, understand, and make sense of human languages in a 
        manner that is valuable.
    - It involves seeral disciplines, including computer science and computational linguistics,
        in an effort to bridge the gap between human communication and computer understanding.
    - Key techniques in (NLP) include syntax tree parsing, entity recognition, and sentiment analysis,
        among others.
    - These techniques help computers to process and anaylze large amounts of natural language data.
    - (NLP) is used in a variety of applications, such as automated chatbots, translation services,
        email filtering, and voice-activated global position systems (GPS).
    - Each application requires the computer to understand the input provided by humans, process
        that data in meaningful way, and if necessary, respong in a language that humans understand.

-  Transformers:
    - Introduced by Google researchers in the seminal 2017 paper " Attention is All You Need".
        transformers use a mechanism known as self-attention to wigh the importance of each word in
        a sentence regardless of it position. Unlike previous models that processed data sequentially,
        Transformers process all words or tokens in parallel, whichsignificantly increases
        efficiency and performance on tasks that require understanding context over long distances within
        text.
    - This architecture avoids recurrence and vonvolutions entirely, relying instead on stacked
        self-attention and point-wise, fully connected layers for both the encoder and the decoder
        components
    - This design allows for more scalable learning and has been fundamental in developing models that
        achieve state-of-the-art results on a variety of (NLP) tasks, including machine translation,
        text summerization, and sentiment analysis.
    - The Transformer's ability to handle sequential data extends beyond text, making it versatile in other
        domains like image processing and even music generation.

- Generative AI models like GPT and BERT use a mechanism called { self-attention } to understand
    and generate contextually relevant text.

- Gernerative Pre-Trained Transformers (GPT):
    - (GPT) are state-of-the-art language models developed by OpenAI that us DL techniques, 
        specifically the transformer architecture, for natural language understanding and generation.
    - These models are first pre-trained on a deverse range of internet text to develop a broad 
        understanding of language structure and context.
    - The pre-training involves unsupervised learning, where the model predicts the next
        word in a setence without human-labeled corrections. This allows GPT models to generate 
        coherent and contextually appropriate text sequences based on the prompt hey are given.
    - Once Pre-trained, GPT models can be fine-tuned on specific tasks such a translation, 
        question-answering, and summerization, enhancing their applicabilitiy across various
        domains.
    - Their ability to generate human-like text and perform language-based tasks has implications
        across fields such as AI-assisted writing, conversational agents, and automated content creation.
    - Each sucessive version fo GPT has been larger and more complex, with GPT-4, the latest iteration,
        containing 175 billion parameters, which signigicantly advances its learning and generative 
        capabilities.

- Tokenization, Word2vec, and BERT
    - Tokenization in NLP involves splitting text into smaller units known as tokens, which
        can be words characters, or subwords. This step is crucial for preparing text for processing
        with various NLP models, as it standardized the initil input into manageable pieces for
        algorithms to process.
    - Word2vec, developed by researchers at Google, is a technique that embeds words into numerical vectors
        using shallow, two-layer NNs. The models are trained to reconstruct the linguistic contexts of words, thereby capturing the relationships and multiple degrees of similarity among them.
    - Meanwhile, Bidirectional Encoder Representations from Transformers (BERT) represent a significant
        advancement in pre-training language representations. Developed also by Google, BERT incorporates
        a transformer architecture that processes words in relation to all the other words in a sentence,
        rather than one-by-one in order. This allows BERT to capture the full context of a word based on all its surroundings leading to a deeper  understanding of language nuances. BERT's ability to handle context from both directions makes it exeptionally powerful for tasks where context is crusial such as question answering and sentiment anaylsis.
    
- 