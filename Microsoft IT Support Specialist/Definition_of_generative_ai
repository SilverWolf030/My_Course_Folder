- What is Generative AI?
    - Artificial Intelligence (AI) imitates human behavior by using machine learning to interact with the 
        environment and exevute tasks without explicit directions on what to output.
    - Generative AI describes a categor of capabilities within AI that create original content.
        Generative AI applications take in natural language input, and return appropriate responses
        in a variety of formats such as natural language, image, or code.

- What are language models?
    - Specialized type of machine learning models that you can use to perform (NLP) tasks including,
        - Determing sentiment or otherwise classifying natural language text.
        - Summarizing text.
        - Comaparing multiple text sources for semantic similarity.
        -Generating new natural language.

- Transformer Models:
    - Builds on and extends some techniques that have been proven successful in modeling vocabularies
        to support NLP tasks - and in particular in generating language.
    -Transformer models are trained with large volumes of text, enabling them to represent the semantic
        relationships between words and use those relationships to determine probable sequences of text that
        make sense.
    - Transformer model architecture consists of two components, or blocks:
        - An Encoder block that creates semantic representations of the training vocabulary.
        - A Decoder block that generates new language sequences.

    {Data} -> {Encoder block} -> {semantic representations} -> {Decoder block} -> {new language sequences}
    
    - The model is trained with a large volume of natural language text, often sourced from the internet 
        or other public sources of text.
    -The sequences of text are broken down into tokens (for example, individual words) and the encoder 
        block processes these token sequences using a technique called attention to determine relationships 
        between tokens (for example, which tokens influence the presence of other tokens in a sequence, 
        different tokens that are commonly used in the same context, and so on.)
    -The output from the encoder is a collection of vectors (multi-valued numeric arrays) in which each 
        element of the vector represents a semantic attribute of the tokens. These vectors are referred to as embeddings.
    -The decoder block works on a new sequence of text tokens and uses the embeddings generated by the encoder 
        to generate an appropriate natural language output.
    -For example, given an input sequence like "When my dog was", the model can use the attention technique to 
        analyze the input tokens and the semantic attributes encoded in the embeddings to predict an appropriate 
        completion of the sentence, such as "a puppy."

    - In practice, the specific implementation of the architecture vary - for example, the Bidrectional 
        Enoder Representations from Transformers (BERT) model developed by Google to support their search
        engine uses only the endoder block, while the Generative Pretained Transformer (GPT) model developed
        by OpenAI uses only the decoder block.

- Tokenization:
    - The first step in training a Transformer model is to decompose the training text into tokens - in other
        words, identify each unique text value. Tokens can be generated for partial words, or combinations
        of words and punctuation.

        - Example: I heard a dog bark loudly at a cat.
            - To tokenize this text, you can identify each discrete word and assign token ids to them.

            - I (1)
            - heard (2)
            - a (3)
            - dog (4)
            - bark (5)
            - loudly (6)
            - at (7)
            - *("a" is already tokenized as 3)*
            - cat (8)

- Embeddings:
    - To create a vocabulary that encapsulates semantic relationships between the tokens, we define contextual
        vectors, known as embeddings, for them.
    - Vectors are multi-valued numeric representations of information, [10,3,1] in which each numeric element 
        represents a particular attribute of the information.
    - For language tokens, each element of a token's vector represents some semantic attribute of the token.
        The specific catagories for the elements of the vectors in a language model are determined during training
        based on how commonly words are used toghethor or in similar context.
    - Vectors represent lines in multidimensional space, describing direction(amplitude) and distance(magnitude)
         along multiple axes. It can be useful to think of the ellements in an embedding vector for a token
          as representing a path in 3- Dimensional space in which the element values indicate the units traveled
          forword/back, left/right and up/down. Overall, the vector describes the direction and distance of the
          path from origin to end.
    - The elements of the tokens in the embeddings space each represent some semantic attribute of the token,
        so that emantically similar tokens should result in vectors that have a similar orientation - in other 
        words they point in the same direction.
    - A technique called cosine similarity is used to determine if two vectors have a similar directions
        (regardless of the direction), and therofor represent simantically linked words.

        Example: Suppose the embeddins for our tokens consist of vectors with three elements:
            
            4 ("dog"): [10,3,2]
            8 ("cat"): [10,3,1]
            9 ("puppy"): [5,2,1]
            10 ("skateboard"): [-3,3,2]
        
        The embedding vectors for "dog" and "puppy" describe a path along an elmost identical direction, 
            which is also fairly similar to the direction for "cat". The embedding vector for "skateboard"
            however describes journey in a very different direction.
        There are multiple ways you can calculate appropriate embeddings for a given set of tokens, including 
            language modeling algorithms like Word2Vec or the encoder block in a transformer model.

- Attention:
    - The encoder and decoder blocks in a transformer model includes multiple layers that form the neural network
        for the model.
    - Attention layers: A technique used to examine a sequence of text tokens and try to quantify the strangth 
        of the relationships between them. In particular, self-attention incolves considering how other tokens
        around one particular token unfluence that token's meaning.
    - In an encoder block, each token is carefully examined in context, and and appropriate encodeing
        is determind for its vector embedding. The vector values are based on the relationships between
        the tokens and other tokens with which it frequently appears. This contextualized approach means that
        the same word might have multiple embeddins depending on the conext in which it's used.
         - for example, " the bark of a tree" means something different to " i heard a dog bark."
    - In a decoder block, attention layers are used to predict the next token in a sequence. For each token
        generated, the model has an attention layer that takes into account the sequence of tokens up to that point.
        The model considers which of the tokens are the most influenctial when considering what the
        next token should be. For example , given the sequence " i heard a dog." the attention layer might
        assign greater weight to the tokens "heard" and "dog" when considering the next word in the sequence:
            
            I heard a dog [bark]
        
    - Remember that the attention layer is working with numeric vector representations of the tokens, not 
        the acutal text. In a decoder, the process starts with a sequence of token embeddings representing the 
        text to be completed. The first thing that happens is that another positional encoding layer adds a 
        value to each embedding to indicate its position in the sequence: 

            [1,5,6,2] (I)
            [2,9,3,1] (heard)
            [3,1,1,2] (a)
            [4,10,3,2] (dog)

    - During training, the goal is to predict the vector for the final token in the sequence based on the
        precding tokens. The attention layer assigns a numberic weight to each token in the sequence so far.
        It uses that value to perform a calculation on the weighted vectors that produces an attention score
        that can be used to calculate a possible vector for the next token.
    - In Practice, a technique called Multi-head attention uses differnet elements of the embeddings to calculate
        multiple attention scores.
    - A Neural Network is then used to evaluate all possible tokens to determine the most probable token with 
        which to continue the sequence. The process continues iteratively for each token in the sequence, with the
        output sequence so far being used regressively as the input for the next iteration - essentially building
        the output on token at a time.
    - During training, the actual sequence of tokens is known - we just mask the ones that come later in the sequence 
        than the toekn position currently being considered.
    - As in any neural network, the predicted value for the token vectors is compared to the actual value of the next 
        vector in the sequence, and the loss is calculated. The weights are then incrementally adjusted to reduce
        the loss and improve the model.
    - When used for inferencing (predicting a new sequence of tokens), the trained attention layer applies weights 
        that predict the most probable token in the model's vocabulary that is semantically algined to the sequence 
        so far.

- Using Language Models:
    - 