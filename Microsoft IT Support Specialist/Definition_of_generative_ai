- What is Generative AI?
    - Artificial Intelligence (AI) imitates human behavior by using machine learning to interact with the 
        environment and exevute tasks without explicit directions on what to output.
    - Generative AI describes a categor of capabilities within AI that create original content.
        Generative AI applications take in natural language input, and return appropriate responses
        in a variety of formats such as natural language, image, or code.

- What are language models?
    - Specialized type of machine learning models that you can use to perform (NLP) tasks including,
        - Determing sentiment or otherwise classifying natural language text.
        - Summarizing text.
        - Comaparing multiple text sources for semantic similarity.
        -Generating new natural language.

- Transformer Models:
    - Builds on and extends some techniques that have been proven successful in modeling vocabularies
        to support NLP tasks - and in particular in generating language.
    -Transformer models are trained with large volumes of text, enabling them to represent the semantic
        relationships between words and use those relationships to determine probable sequences of text that
        make sense.
    - Transformer model architecture consists of two components, or blocks:
        - An Encoder block that creates semantic representations of the training vocabulary.
        - A Decoder block that generates new language sequences.

    {Data} -> {Encoder block} -> {semantic representations} -> {Decoder block} -> {new language sequences}
    
    - The model is trained with a large volume of natural language text, often sourced from the internet 
        or other public sources of text.
    -The sequences of text are broken down into tokens (for example, individual words) and the encoder 
        block processes these token sequences using a technique called attention to determine relationships 
        between tokens (for example, which tokens influence the presence of other tokens in a sequence, 
        different tokens that are commonly used in the same context, and so on.)
    -The output from the encoder is a collection of vectors (multi-valued numeric arrays) in which each 
        element of the vector represents a semantic attribute of the tokens. These vectors are referred to as embeddings.
    -The decoder block works on a new sequence of text tokens and uses the embeddings generated by the encoder 
        to generate an appropriate natural language output.
    -For example, given an input sequence like "When my dog was", the model can use the attention technique to 
        analyze the input tokens and the semantic attributes encoded in the embeddings to predict an appropriate 
        completion of the sentence, such as "a puppy."

    - In practice, the specific implementation of the architecture vary - for example, the Bidrectional 
        Enoder Representations from Transformers (BERT) model developed by Google to support their search
        engine uses only the endoder block, while the Generative Pretained Transformer (GPT) model developed
        by OpenAI uses only the decoder block.

- Tokenization:
    - 